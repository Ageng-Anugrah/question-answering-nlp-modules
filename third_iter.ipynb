{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ddc18d-0723-416f-922c-187d1ff7ef6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a6aa06-273f-469f-bc06-ce222a505d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "from gensim.models import FastText\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from pyemd import emd\n",
    "import stanza\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "import nltk.data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc98812-3b57-4b07-9206-8b6675d1375a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd64be75-81a3-40c5-8b26-09b962395feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('gensim').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7015f34-8050-4df2-8b14-6194d45b47bf",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "c71525c8-926a-4a0e-9230-d7fc48801f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squad_test = pd.read_csv(\"squad-test.csv\",converters = {'answer': ast.literal_eval},index_col=0)\n",
    "tydiqa_test = pd.read_csv(\"tydiqa-test.csv\", converters = {'answer': ast.literal_eval},index_col=0)\n",
    "idkmrc_test = pd.read_csv(\"idkmrc-test.csv\",converters = {'answer': ast.literal_eval},index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41102629-1e98-481f-8506-373a79b95c7d",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87719e-bcbf-47ff-ac61-d66f69e0b8d7",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4067a-a67f-469a-b79f-32ef1125849c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('id', processors='tokenize,lemma')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokenized = [word.lemma for sentence in doc.sentences for word in sentence.words if word.lemma != None]\n",
    "    try:\n",
    "        text = [w for w in tokenized if w not in string.punctuation]\n",
    "    except TypeError:\n",
    "        print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa152b88-253d-44bb-aa3e-98d40a2f20f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## WordMoverDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "cda6ba37-f840-4c84-ab7c-3c95982a3827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models._fasttext_bin:loading 457946 words for fastText model from fasttext-4B-id-uncased/fasttext.4B.id.300.epoch5_uncased_no-oov_pos-idn_uncased.bin\n",
      "INFO:gensim.utils:FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-07-01T09:32:48.467754', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "INFO:gensim.models.word2vec:Updating model with new vocabulary\n",
      "INFO:gensim.utils:FastText lifecycle event {'msg': 'added 457946 new unique words (100.00% of original 457946) and increased the count of 0 pre-existing words (0.00% of original 457946)', 'datetime': '2023-07-01T09:32:50.136800', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 457946 items\n",
      "INFO:gensim.models.word2vec:sample=0.0001 downsamples 1 most-common words\n",
      "INFO:gensim.utils:FastText lifecycle event {'msg': 'downsampling leaves estimated 4690649.603462987 word corpus (99.7%% of prior 4703046)', 'datetime': '2023-07-01T09:32:52.158330', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:FastText lifecycle event {'msg': 'loaded (457946, 100) weight matrix for fastText model from fasttext-4B-id-uncased/fasttext.4B.id.300.epoch5_uncased_no-oov_pos-idn_uncased.bin', 'datetime': '2023-07-01T09:32:55.413329', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'load_fasttext_format'}\n"
     ]
    }
   ],
   "source": [
    "model_wm_1 = FastText.load_fasttext_format(\"fasttext-4B-id-uncased/fasttext.4B.id.300.epoch5_uncased_no-oov_pos-idn_uncased.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "e95feab7-7dec-459e-a594-10c4260a64da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_wm_1 = model_wm_1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "8a1e9bd9-ce1b-409c-b3cc-589cfa3c679c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models._fasttext_bin:loading 5196198 words for fastText model from fasttext.4B.id.300.epoch5.uncased.bin\n",
      "INFO:gensim.utils:FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-07-01T09:33:10.949393', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "INFO:gensim.models.word2vec:Updating model with new vocabulary\n",
      "INFO:gensim.utils:FastText lifecycle event {'msg': 'added 5196198 new unique words (100.00% of original 5196198) and increased the count of 0 pre-existing words (0.00% of original 5196198)', 'datetime': '2023-07-01T09:33:33.032344', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:FastText lifecycle event {'msg': 'loaded (5196198, 100) weight matrix for fastText model from fasttext.4B.id.300.epoch5.uncased.bin', 'datetime': '2023-07-01T09:34:35.566779', 'gensim': '4.3.1', 'python': '3.8.10 (default, Mar 13 2023, 10:26:41) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-90-generic-x86_64-with-glibc2.29', 'event': 'load_fasttext_format'}\n"
     ]
    }
   ],
   "source": [
    "model_wm_2 = FastText.load_fasttext_format(\"fasttext.4B.id.300.epoch5.uncased.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "dc4bf81b-fbbf-42c3-9822-dc709f8f5371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_wm_2 = model_wm_2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "6a2ecf33-7d1d-4aef-8808-f467fad40cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_distance(text_1, text_2, model= model_wm_1, model_2=model_wm_2):\n",
    "    a_prep = preprocess_text(text_1)\n",
    "    b_prep = preprocess_text(text_2)\n",
    "    dis_1 = model_wm_1.wmdistance(a_prep, b_prep)\n",
    "    if dis_1 == inf:\n",
    "        return model_wm_2.wmdistance(a_prep, b_prep)\n",
    "    return dis_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12e9b9-f2a6-4d57-bdf6-e8b03420e54d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_distance(\"kipas angin\",\"angin kipas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a76515-7a09-4f17-9b6c-c95a486a3545",
   "metadata": {},
   "source": [
    "## Split Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef12698c-b3c7-4bba-9c21-ab2986220de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def split_sentences(text):\n",
    "    # Menggunakan PunktSentenceTokenizer untuk Bahasa Indonesia\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d474d-cc21-4974-b326-b3bb5ad9cb11",
   "metadata": {},
   "source": [
    "## NER, TIMEX, POSTAG, Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "307801b3-e9ba-4b39-bbce-0c1a75102700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"model/indobert-large-p2-finetuned-ner\")\n",
    "modul_ner = AutoModelForTokenClassification.from_pretrained(\"model/indobert-large-p2-finetuned-ner\")\n",
    "tokenizer_pos = AutoTokenizer.from_pretrained(\"model/indobert-large-p2-finetuned-pos\")\n",
    "modul_pos = AutoModelForTokenClassification.from_pretrained(\"model/indobert-large-p2-finetuned-pos\")\n",
    "tokenizer_chunking = AutoTokenizer.from_pretrained(\"model/indobert-large-p2-finetuned-chunking\")\n",
    "modul_chunking = AutoModelForTokenClassification.from_pretrained(\"model/indobert-large-p2-finetuned-chunking\")\n",
    "tokenizer_timex = AutoTokenizer.from_pretrained(\"model/indobert-large-p2-finetuned-indotimex\")\n",
    "modul_timex = AutoModelForTokenClassification.from_pretrained(\"model/indobert-large-p2-finetuned-indotimex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8270edda-1f4a-4e8e-9ca8-110df3ed0c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, sentence):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(sentence.split(),\n",
    "                    is_split_into_words = True,\n",
    "                    return_offsets_mapping=True, \n",
    "                    return_tensors=\"pt\",\n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=512)\n",
    "    \n",
    "    model.to(device)\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    outputs = model(ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "    active_logits = logits.view(-1, model.num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) \n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [model.config.id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions)) \n",
    "\n",
    "    prediction = []\n",
    "    for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "        #only predictions on first word pieces are important\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            prediction.append(token_pred[1])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return sentence.split(), prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed015c3b-ce6e-4d19-969f-287fe0846fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tag_predictions(model, tokenizer, sentences):\n",
    "    predictions = []\n",
    "    for sentence in sentences:\n",
    "        pred = predict(model, tokenizer, sentence)\n",
    "        predictions.append(list(zip(*pred)))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7b45c-ea25-435c-9cae-560962dc632c",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e373661-6a4d-4593-acba-2e99b5cb52f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/Tugas Akhir/indolem/dependency_parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2eb3ee8-6d95-4f89-862f-ff1d62c86495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dep_parser import get_dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53f485f-77ab-444e-829b-df86611c3051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_dependency(words, tags):\n",
    "    a, b = words, tags \n",
    "    zipped_word_label = []\n",
    "    instance2indexpos =  {\n",
    "            \"_PAD_POS\": 0,\n",
    "            \"_ROOT_POS\": 1,\n",
    "            \"_END_POS\": 2,\n",
    "            \"PROPN\": 3,\n",
    "            \"AUX\": 4,\n",
    "            \"DET\": 5,\n",
    "            \"NOUN\": 6,\n",
    "            \"PRON\": 7,\n",
    "            \"VERB\": 8,\n",
    "            \"ADP\": 9,\n",
    "            \"PUNCT\": 10,\n",
    "            \"ADV\": 11,\n",
    "            \"CCONJ\": 12,\n",
    "            \"SCONJ\": 13,\n",
    "            \"NUM\": 14,\n",
    "            \"ADJ\": 15,\n",
    "            \"PART\": 16,\n",
    "            \"SYM\": 17,\n",
    "            \"X\": 18\n",
    "        }\n",
    "    mapping = {\n",
    "        \"PRP\": \"PRON\",\n",
    "        \"NN\": \"NOUN\",\n",
    "        \"NNP\": \"PROPN\",\n",
    "        \"CD\": \"NUM\",\n",
    "        \"VB\": \"VERB\",\n",
    "        \"RB\": \"ADV\",\n",
    "        \"DT\": \"DET\"\n",
    "    }     \n",
    "    for i, j in zip(a,b):\n",
    "        j = mapping.get(j[2:], j[2:])\n",
    "        if j not in instance2indexpos.keys():\n",
    "            j = \"X\"\n",
    "        zipped_word_label.append((i, j))\n",
    "        \n",
    "    return get_dependency(zipped_word_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fba1a7-744e-4cff-83bc-e3fb1ebc5531",
   "metadata": {},
   "source": [
    "## Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1466334e-5ed0-4fcd-bc0f-a2e78bf8acdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"irwanto/indocoref/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a12a73c-40fe-4b86-87d4-92d84d1a460b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from core_2 import predict as predict_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251c140-5e62-40d6-8994-9dd5e09737a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = predict_coref('{M1:jenis=\"\" Orang Eropa pertama} yang melakukan perjalanan sepanjang {M2:jenis=\"\" Sungai Amazon} adalah {M3:jenis=\"\" Francisco de Orellana} pada tahun 1542. {M6:jenis=\"\" Dia} lahir di Semarang. {M4:jenis=\"\" Wartawan BBC Unnatural Histories} menyajikan bukti bahwa Orellana, bukannya membesar-besarkan klaimnya seperti yang diduga sebelumnya, adalah benar dalam pengamatannya bahwa peradaban kompleks berkembang di sepanjang {M5:jenis=\"\" Amazon}. di tahun 1540-an. Diyakini bahwa peradaban itu kemudian dihancurkan oleh penyebaran penyakit dari Eropa, seperti cacar. Sejak tahun 1970-an, banyak geoglyph telah ditemukan di tanah gundul yang berasal dari tahun 0-1250 M, melanjutkan klaim tentang peradaban Pra-Kolombia. Ondemar Dias terakreditasi dengan pertama kali menemukan geoglyph pada tahun 1977 dan Alceu Ranzi dengan melanjutkan penemuan mereka setelah terbang di atas Acre. Wartawan BBC Unnatural Histories menyajikan bukti bahwa hutan hujan Amazon, daripada menjadi hutan belantara yang murni, telah dibentuk oleh manusia setidaknya selama 11.000 tahun melalui praktik-praktik seperti berkebun dan terra preta.', 'Orang Eropa pertama yang melakukan perjalanan sepanjang Sungai Amazon adalah Francisco de Orellana pada tahun 1542. Wartawan BBC Unnatural Histories menyajikan bukti bahwa Orellana, bukannya membesar-besarkan klaimnya seperti yang diduga sebelumnya, adalah benar dalam pengamatannya bahwa peradaban kompleks berkembang di sepanjang Amazon. di tahun 1540-an. Diyakini bahwa peradaban itu kemudian dihancurkan oleh penyebaran penyakit dari Eropa, seperti cacar. Sejak tahun 1970-an, banyak geoglyph telah ditemukan di tanah gundul yang berasal dari tahun 0-1250 M, melanjutkan klaim tentang peradaban Pra-Kolombia. Ondemar Dias terakreditasi dengan pertama kali menemukan geoglyph pada tahun 1977 dan Alceu Ranzi dengan melanjutkan penemuan mereka setelah terbang di atas Acre. Wartawan BBC Unnatural Histories menyajikan bukti bahwa hutan hujan Amazon, daripada menjadi hutan belantara yang murni, telah dibentuk oleh manusia setidaknya selama 11.000 tahun melalui praktik-praktik seperti berkebun dan terra preta.')\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce76d01-371e-4f0b-bd37-b30dd25046cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mention Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3adc426c-063e-400e-9fc7-105191adf5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mentions(row):\n",
    "    mentions = []\n",
    "    for idx_s, sentence in enumerate(row[\"context_features\"]+row[\"question_features\"]):\n",
    "        state = 0\n",
    "        text = \"\"\n",
    "        prev_pos_tag = None\n",
    "        for idx_w, word in enumerate(sentence):\n",
    "            label_pos = word[\"pos\"][2:]\n",
    "            label_ner = word[\"ner\"]\n",
    "            if state == 0:\n",
    "                if label_pos in [\"NNP\", \"PRP\", \"NN\", \"PR\"] and label_ner == \"O\":\n",
    "                    state = 1\n",
    "                elif label_ner[0] == \"B\":\n",
    "                    state = 2\n",
    "\n",
    "            if state == 1:\n",
    "                if text == \"\":\n",
    "                    text += word[\"kata\"] + \" \"\n",
    "                    prev_pos_tag = label_pos\n",
    "                    continue\n",
    "                if label_pos != prev_pos_tag:\n",
    "                    mentions.append((text, \"\"))\n",
    "                    state = 0\n",
    "                    text = \"\"\n",
    "                    prev_pos_tag = None\n",
    "                else:\n",
    "                    text += word[\"kata\"] + \" \"\n",
    "            elif state == 2:\n",
    "                if label_ner[0] == \"B\":\n",
    "                    if text == \"\":\n",
    "                        text += word[\"kata\"] + \" \"\n",
    "                    else:\n",
    "                        mentions.append((text, \"\"))\n",
    "                        text = word[\"kata\"] + \" \"\n",
    "                if label_ner[0] == \"I\":\n",
    "                    text += word[\"kata\"] + \" \"\n",
    "                if label_ner[0] == \"O\" and text != \"\":       \n",
    "                    prev_label = sentence[idx_w-1][\"ner\"]\n",
    "                    if \"PER\" in prev_label:\n",
    "                        mentions.append((text, \"named-entity person\"))\n",
    "                    elif \"PLA\" in prev_label:\n",
    "                        mentions.append((text, \"named-entity place\"))\n",
    "                    elif \"ORG\" in prev_label:\n",
    "                        mentions.append((text, \"named-entity organisasi\"))\n",
    "                    text = \"\"\n",
    "                    state = 0\n",
    "\n",
    "        if text != \"\":\n",
    "            mentions.append((text, \"\"))\n",
    "    \n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73332ded-12ee-4b23-a7db-5a154bab4319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def annotate_mentions(text_ori, mentions):\n",
    "    sentence_copy = text_ori\n",
    "    i = 0\n",
    "    for mention, jenis in mentions:\n",
    "        # print(i)\n",
    "        pattern = fr\"(?<!\\{{[^}}]*){re.escape(mention[:-1])}(?!.*?\\}})\"\n",
    "        # print(pattern)\n",
    "        rep = f\"{{M{i}:jenis=\\\"{jenis}\\\" {mention[:-1]}}}\"\n",
    "        sentence_copy = re.sub(pattern, rep, sentence_copy, 1)\n",
    "        i +=1\n",
    "        \n",
    "    return sentence_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9dfe242f-8398-41a0-8073-c1eab4b327a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_coref(row):\n",
    "    text_ori = row[\"context\"] + \" \" + row[\"question\"]\n",
    "    text_ori_2 = row[\"context\"]\n",
    "    mentions = get_mentions(row)\n",
    "    annotated = annotate_mentions(text_ori, mentions)\n",
    "    annotated_2 = annotate_mentions(text_ori_2, mentions)\n",
    "    try: \n",
    "        predicted = predict_coref(annotated, text_ori)\n",
    "    except:\n",
    "        return text_ori_2\n",
    "        \n",
    "    entities = row[\"content_entities\"]\n",
    "    flag = False\n",
    "    annotated = annotated_2\n",
    "    for k, v in predicted.items():        \n",
    "        is_included_ner = False\n",
    "        ner = None\n",
    "        for i in v:\n",
    "            if flag:\n",
    "                break\n",
    "            for j in entities:\n",
    "                if j.lower() in i[1].lower():\n",
    "                    is_included_ner = True\n",
    "                    ner = remove_punctuation(j)\n",
    "                    flag = True\n",
    "                    break\n",
    "                \n",
    "        if not is_included_ner:\n",
    "            for i in v:\n",
    "                pattern = \"(\\{M\" + str(i[0]) + \":.*?\\})\"\n",
    "                annotated = re.sub(pattern, i[1], annotated)\n",
    "            continue\n",
    "        \n",
    "        for i in v:\n",
    "            pattern = \"(\\{M\" + str(i[0]) + \":.*?\\})\"\n",
    "            annotated = re.sub(pattern, ner, annotated)\n",
    "            \n",
    "        \n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9ebd1-237a-4124-87e2-b0bc6bf8812a",
   "metadata": {},
   "source": [
    "## Get EAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8db1642d-010e-4b81-a7ea-ab95eb614b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PER = \"PERSON\"\n",
    "PLA = \"PLACE\"\n",
    "ORG = \"ORGANISATION\"\n",
    "TIME = \"TIME\"\n",
    "eat_dict = {\n",
    "    \"kapan\": [TIME],\n",
    "    \"kapankah\": [TIME],\n",
    "    \"dimana\": [PLA, \"NP\"],\n",
    "    \"mana\": [PLA, \"NP\"],\n",
    "    \"darimana\": [PLA, \"NP\"],\n",
    "    \"darimanakah\": [PLA, \"NP\"],\n",
    "    \"dimanakah\": [PLA, \"NP\"],\n",
    "    \"manakah\": [PLA, \"NP\"],\n",
    "    \"siapa\": [PER, ORG],\n",
    "    \"siapakah\": [PER, ORG],\n",
    "    \"apa\": [\"NP\"],\n",
    "    \"apakah\": [\"NP\"],\n",
    "    \"kenapa\": [\"NP\", \"VP\"],\n",
    "    \"mengapa\": [\"NP\", \"VP\"],\n",
    "    \"berapa\": [\"CD\"],\n",
    "    \"berapakah\": [\"CD\"],\n",
    "    \"seberapa\": [\"CD\"],\n",
    "    \"beberapa\": [\"VP\", \"NP\"],\n",
    "    \"bagaimana\": [\"VP\", \"NP\"],\n",
    "    \"bagaimanakah\": [\"VP\", \"NP\"],\n",
    "    \"lainnya\": [\"NP\", \"PLA\", \"ORG\", \"PER\", TIME, \"CD\"]\n",
    "}\n",
    "\n",
    "eat_dict_2 = {\n",
    "    \"kapan\": [TIME],\n",
    "    \"kapankah\": [TIME],\n",
    "    \"dimana\": [PLA],\n",
    "    \"di mana\": [PLA],\n",
    "    \"darimanakah\": [PLA],\n",
    "    \"dari manakah\": [PLA],\n",
    "    \"darimana\": [PLA],\n",
    "    \"dari mana\": [PLA],\n",
    "    \"dimanakah\": [PLA],\n",
    "    \"di manakah\": [PLA],\n",
    "    \"manakah\": [PLA],\n",
    "    \"kemana\": [PLA],\n",
    "    \"ke mana\": [PLA],\n",
    "    \"kemanakah\": [PLA],\n",
    "    \"ke manakah\": [PLA],\n",
    "    \"siapa\": [PER, ORG],\n",
    "    \"siapakah\": [PER, ORG],\n",
    "    \"lainnya\": None\n",
    "}\n",
    "\n",
    "eat_dict_3 = {\n",
    "    \"kapan\": [TIME, \"CD\", \"NP\"],\n",
    "    \"kapankah\": [TIME, \"CD\", \"NP\"],\n",
    "    \"dimana\": [PLA, \"NP\"],\n",
    "    \"di mana\": [PLA, \"NP\"],\n",
    "    \"darimanakah\": [PLA, \"NP\"],\n",
    "    \"dari manakah\": [PLA, \"NP\"],\n",
    "    \"darimana\": [PLA, \"NP\"],\n",
    "    \"dari mana\": [PLA, \"NP\"],\n",
    "    \"dimanakah\": [PLA, \"NP\"],\n",
    "    \"di manakah\": [PLA, \"NP\"],\n",
    "    \"manakah\": [PLA, \"NP\"],\n",
    "    \"mana\": [PLA, \"NP\"],\n",
    "    \"kemana\": [PLA, \"NP\"],\n",
    "    \"ke mana\": [PLA, \"NP\"],\n",
    "    \"kemanakah\": [PLA, \"NP\"],\n",
    "    \"ke manakah\": [PLA, \"NP\"],\n",
    "    \"siapa\": [PER, ORG, \"NP\"],\n",
    "    \"siapakah\": [PER, ORG, \"NP\"],\n",
    "    \"lainnya\": None\n",
    "}\n",
    "\n",
    "eat_dict_4 = {\n",
    "    \"apa\": [\"NP\",],\n",
    "    \"apakah\": [\"NP\",],\n",
    "    \"kenapa\": [\"NP\", \"VP\",],\n",
    "    \"mengapa\": [\"NP\", \"VP\"],\n",
    "    \"berapa\": [\"CD\", \"OD\", \"NP\"],\n",
    "    \"berapakah\": [\"CD\", \"OD\", \"NP\"],\n",
    "    \"seberapa\": [\"CD\", \"OD\", \"NP\"],\n",
    "    \"beberapa\": [\"VP\", \"NP\"],\n",
    "    \"bagaimana\": [\"VP\", \"NP\", \"VB\"],\n",
    "    \"bagaimanakah\": [\"VP\", \"NP\"],\n",
    "    \"lainnya\": None\n",
    "}\n",
    "\n",
    "eat_dict_5 = {**eat_dict_3, **eat_dict_4,\n",
    "    \"lainnya\": [\"NP\", \"PLA\", \"ORG\", \"PER\", TIME, \"CD\"]\n",
    "} \n",
    "\n",
    "eat_dict_6 = {\n",
    "    \"kapan\": [\"CD\"],\n",
    "    \"kapankah\": [\"CD\"],\n",
    "    \"dimana\": [\"NP\"],\n",
    "    \"di mana\": [\"NP\"],\n",
    "    \"darimanakah\": [\"NP\"],\n",
    "    \"dari manakah\": [\"NP\"],\n",
    "    \"darimana\": [\"NP\"],\n",
    "    \"dari mana\": [\"NP\"],\n",
    "    \"dimanakah\": [\"NP\"],\n",
    "    \"di manakah\": [\"NP\"],\n",
    "    \"kemana\": [\"NP\"],\n",
    "    \"ke mana\": [\"NP\"],\n",
    "    \"kemanakah\": [\"NP\"],\n",
    "    \"ke manakah\": [\"NP\"],\n",
    "    \"siapa\": [\"NP\"],\n",
    "    \"siapakah\": [\"NP\"],\n",
    "    \"lainnya\": None\n",
    "}\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def get_eat(question, eat_dict):\n",
    "    eat = None\n",
    "    removed_punct = remove_punctuation(question).lower()\n",
    "    for k in eat_dict:\n",
    "        pattern = re.compile(r'\\b' + re.escape(k) + r'\\b')\n",
    "        match = re.search(pattern, removed_punct)\n",
    "        if match:\n",
    "            eat = eat_dict[k]\n",
    "            break\n",
    "            \n",
    "    if k == \"lainnya\":        \n",
    "        eat = eat_dict[\"lainnya\"]\n",
    "            \n",
    "    return k, eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85585f88-cbf9-4107-b2ab-fdcfa6a9b157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_f1_avg(i, j, k):\n",
    "    count = {\n",
    "        \"kapan\": [0,0,0],\n",
    "        \"dimana\": [0,0,0],\n",
    "        \"siapa\": [0,0,0],\n",
    "        \"apa\": [0,0,0],\n",
    "        \"mengapa\": [0,0,0],\n",
    "        \"kenapa\": [0,0,0],\n",
    "        \"berapa\": [0,0,0],\n",
    "        \"bagaimana\": [0,0,0],\n",
    "        \"lainnya\": [0,0,0],\n",
    "        \"beberapa\": [0,0,0]\n",
    "    }\n",
    "    for x, y, z in zip(i, j, k):\n",
    "        if x == [TIME, \"CD\", \"NP\"]:\n",
    "            count[\"kapan\"][0] += 1\n",
    "            count[\"kapan\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"kapan\"][2] += 1\n",
    "        elif x == [PLA, \"NP\"]:\n",
    "            count[\"dimana\"][0] += 1\n",
    "            count[\"dimana\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"dimana\"][2] += 1\n",
    "        elif x==[PER, ORG, \"NP\"]:\n",
    "            count[\"siapa\"][0] += 1\n",
    "            count[\"siapa\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"siapa\"][2] += 1\n",
    "        elif x == [\"NP\"]:\n",
    "            count[\"apa\"][0] += 1\n",
    "            count[\"apa\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"apa\"][2] += 1\n",
    "        elif y == \"mengapa\":\n",
    "            count[\"mengapa\"][0] += 1\n",
    "            count[\"mengapa\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"mengapa\"][2] += 1\n",
    "        elif y == \"kenapa\":\n",
    "            count[\"kenapa\"][0] += 1\n",
    "            count[\"kenapa\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"kenapa\"][2] += 1\n",
    "        elif x == [\"CD\", \"OD\", \"NP\"]:\n",
    "            count[\"berapa\"][0] += 1\n",
    "            count[\"berapa\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"berapa\"][2] += 1\n",
    "        elif y == \"beberapa\":\n",
    "            count[\"beberapa\"][0] += 1\n",
    "            count[\"beberapa\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"beberapa\"][2] += 1\n",
    "        elif y in [\"bagaimana\", \"bagaimanakah\"]:\n",
    "            count[\"bagaimana\"][0] += 1\n",
    "            count[\"bagaimana\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"bagaimana\"][2] += 1\n",
    "        else:\n",
    "            count[\"lainnya\"][0] += 1\n",
    "            count[\"lainnya\"][1] += z\n",
    "            if z == 1:\n",
    "                count[\"lainnya\"][2] += 1\n",
    "        \n",
    "            \n",
    "    for k, v in count.items():\n",
    "        if v[0] != 0:\n",
    "            print(f\"{k}: {v[1]/v[0]} {v[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3f3b0-6c21-46f8-af56-117cb827be3b",
   "metadata": {},
   "source": [
    "## Get Candidate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "135243b2-2d17-4c22-9128-5768ffd4dc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_chunk_tag = [\"ADJP\", \"ADVP\", \"INTJ\", \"NP\", \"PP\", \"PRT\", \"SBAR\", \"UCP\", \"VP\"]\n",
    "list_ner = [\"PERSON\", \"PLACE\", \"ORGANISATION\"]\n",
    "list_pos = [\"CD\", \"OD\", \"VB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f512ef8-bafd-4e3d-8d4b-92726e6487e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_candidate_answer_2(eat, feature):\n",
    "    candidate_answers = []\n",
    "    if eat != None:\n",
    "        for i in eat:\n",
    "            if i in list_chunk_tag:\n",
    "                count_sent = 0\n",
    "                for sentence in feature:\n",
    "                    count = 0\n",
    "                    for word in sentence:\n",
    "                        tag_temp = word[\"chunk\"][2:]\n",
    "                        if tag_temp in eat:\n",
    "                            candidate_answers.append((word[\"kata\"],word[\"chunk\"],count, count_sent))\n",
    "                        count+=1\n",
    "                    count_sent += 1\n",
    "            if i == TIME:\n",
    "                count_sent = 0\n",
    "                for sentence in feature:\n",
    "                    count = 0\n",
    "                    for word in sentence:\n",
    "                        if word[\"timex\"] != \"O\":\n",
    "                            candidate_answers.append((word[\"kata\"],word[\"timex\"],count, count_sent))\n",
    "                        count+=1\n",
    "                    count_sent+=1 \n",
    "            if i in list_ner:\n",
    "                count_sent = 0\n",
    "                for sentence in feature:\n",
    "                    count = 0\n",
    "                    for word in sentence:\n",
    "                        if i in word[\"ner\"]:\n",
    "                            candidate_answers.append((word[\"kata\"],word[\"ner\"],count, count_sent))\n",
    "                        count+=1\n",
    "                    count_sent += 1\n",
    "            if i in list_pos:\n",
    "                count_sent = 0\n",
    "                for sentence in feature:\n",
    "                    count = 0\n",
    "                    for word in sentence:\n",
    "                        if i in word[\"pos\"]:\n",
    "                            candidate_answers.append((word[\"kata\"],word[\"pos\"],count, count_sent))\n",
    "                        count+=1\n",
    "                    count_sent += 1\n",
    "    return candidate_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab1cd1-bdd1-4870-81b2-4ab3ecd5f32d",
   "metadata": {},
   "source": [
    "## Get Final Candidate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97025695-2bca-4c85-a910-8da4bbdcba83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_longest_span(lst):\n",
    "    terpanjang = ''\n",
    "    for string in lst:\n",
    "        if len(string) > len(terpanjang):\n",
    "            terpanjang = string\n",
    "    return terpanjang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15efd7b1-629c-4e89-894e-2585cc5d6629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_final_candidate_answers_2(candidates, features):\n",
    "    final = {}\n",
    "    text = \"\"\n",
    "    index_temp = None\n",
    "    index_s_temp = None\n",
    "    count = 0\n",
    "\n",
    "    for i in candidates:\n",
    "        if i[1][0] == \"B\":\n",
    "            if text == \"\":\n",
    "                index_temp = i[2]\n",
    "                index_s_temp = i[3]\n",
    "                text += i[0]\n",
    "                count = 0\n",
    "            else:\n",
    "                if i[2] == index_temp + count and i[3] == index_s_temp:\n",
    "                    text += \" \" + i[0]\n",
    "                    count += 1\n",
    "                    continue\n",
    "                if i[2] == index_temp + count + 1 and features[i[3]][i[2]-1][\"pos\"] == \"B-CC\" and i[3] == index_s_temp:\n",
    "                    text += \" \" + features[i[3]][i[2]-1][\"kata\"] + \" \" + i[0]\n",
    "                    count += 2\n",
    "                    continue\n",
    "                final.setdefault((index_s_temp, index_temp), []).append(text)\n",
    "                text = \"\"\n",
    "                text += i[0]\n",
    "                index_temp = i[2]\n",
    "                index_s_temp = i[3]\n",
    "                count = 0\n",
    "        elif i[1][0] == \"I\" and i[3] == index_s_temp and i[2] == index_temp + count:\n",
    "            text += \" \" + i[0]\n",
    "        elif i[1][0] == \"I\":\n",
    "            if text == \"\":\n",
    "                index_temp = i[2]\n",
    "                index_s_temp = i[3]\n",
    "                text += i[0]\n",
    "                count = 0\n",
    "            else:\n",
    "                final.setdefault((index_s_temp, index_temp), []).append(text)\n",
    "                text = \"\"\n",
    "                text += i[0]\n",
    "                index_temp = i[2]\n",
    "                index_s_temp = i[3]\n",
    "                count = 0\n",
    "        count += 1\n",
    "\n",
    "    \n",
    "    if text != \"\":\n",
    "        final.setdefault((index_s_temp, index_temp), []).append(text)\n",
    "        \n",
    "    final = split_dictionary(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67b46188-6697-4368-b420-202319ecf848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dictionary(dictionary):\n",
    "    result = {}\n",
    "    \n",
    "    for key, values in dictionary.items():\n",
    "        s, t = key\n",
    "        \n",
    "        for i, value in enumerate(values):\n",
    "            new_key = (s, t, t + len(value.split()))\n",
    "            result[new_key] = value\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Contoh penggunaan\n",
    "dictionary = {(0, 1): [\"Kata\"], (1, 2): [\"Kata Siapa\", \"Kata Saya Kah\"]}\n",
    "new_dictionary = split_dictionary(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c792a-eb98-4f96-81f8-393091da254a",
   "metadata": {},
   "source": [
    "## Filter WH from Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f06066d-9a2c-4f43-93a8-0d8c89a19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_wh(pair_words_tags):\n",
    "    words = []\n",
    "    for word, tag in pair_words_tags:\n",
    "        if \"WH\" not in tag and word.lower() not in eat_dict.keys() and \"Z\" not in tag:\n",
    "            words.append((word,tag))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba9b04-dd45-4974-9a73-f0bff833f0d9",
   "metadata": {},
   "source": [
    "## Get Question Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "393b2cc9-6e17-4bed-86f9-27ef7f5bf075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_question_key(pairs_pos):\n",
    "    output = \" \".join([word for word, _ in pairs_pos])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5077507e-de92-4dfc-a46c-f2a1cae24bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conjuction_dict = {\n",
    "    \"adalah\": [\"apa\", \"apakah\", \"siapa\", \"siapakah\"],\n",
    "    \"di\": [\"dimana\", \"dimanakah\", \"di mana\", \"di manakah\"],\n",
    "    \"dari\": [\"darimanakah\", \"darimana\", \"dari manakah\", \"dari mana\"],\n",
    "    \"karena\": [\"mengapa\", \"kenapa\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c4a6a-afde-4857-9f95-63f4bb78d691",
   "metadata": {},
   "source": [
    "## Get Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00e5faf6-2fa2-42b6-aa6a-6618c7604897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_conjuction(kata_tanya):\n",
    "    for k, v in conjuction_dict.items():\n",
    "        if kata_tanya in v:\n",
    "            return k\n",
    "    return \"\"\n",
    "        \n",
    "def get_final_answer(passage_sentences, kata_tanya, question_key, candidate_answer, debug):\n",
    "    dis_lowest = 9999\n",
    "    sen = \"\"\n",
    "\n",
    "    try:\n",
    "        for key, val in candidate_answer.items():\n",
    "            conj = get_conjuction(kata_tanya)\n",
    "            target = question_key + \" \" + conj + \" \" + val\n",
    "            target_2 = passage_sentences[key[0]]\n",
    "            dis = get_distance(target, target_2)\n",
    "            if dis < dis_lowest and dis <= 1.3:\n",
    "                dis_lowest = dis\n",
    "                sen = val\n",
    "            if debug:\n",
    "                print('\\033[1m' + \"Target 1:\" + \"\\033[0m\")\n",
    "                print(target)\n",
    "                print('\\033[1m' + \"Target 2:\" + \"\\033[0m\")\n",
    "                print(target_2)\n",
    "                print(f\"Distance: {dis}\\n\")\n",
    "    except AttributeError:\n",
    "        if len(candidate_answer) == 1:\n",
    "            return list(candidate_answer)[0]\n",
    "    return sen\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce6fe9-950e-46f3-998e-36538af42408",
   "metadata": {},
   "source": [
    "## Get Content Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "820ed164-e57e-4344-926d-13fe771cbb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_tags(tagged_list):\n",
    "    merged_list = []\n",
    "    current_phrase = \"\"\n",
    "    current_tag = \"\"\n",
    "\n",
    "    for word, tag in tagged_list:\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_phrase:\n",
    "                merged_list.append(current_phrase.strip())\n",
    "            current_phrase = word\n",
    "            current_tag = tag.split(\"-\")[1]\n",
    "        elif tag.startswith(\"I-\") and current_tag == tag.split(\"-\")[1]:\n",
    "            current_phrase += \" \" + word\n",
    "\n",
    "    if current_phrase:\n",
    "        merged_list.append(current_phrase.strip())\n",
    "\n",
    "    return merged_list\n",
    "\n",
    "def get_content_word_entity_and_np(row):\n",
    "    entities = []\n",
    "    for token in row[\"question_features\"][0]:\n",
    "            if token[\"ner\"] != \"O\" or \"NP\" in token[\"chunk\"] and token[\"pos\"] != \"B-WH\" and token[\"kata\"].lower() not in eat_dict_4.keys():\n",
    "                entities.append(token[\"kata\"])\n",
    "    return entities\n",
    "\n",
    "def get_content_word_entity(row):\n",
    "    entities = []\n",
    "    for token in row[\"question_features\"][0]:\n",
    "            if token[\"ner\"] != \"O\":\n",
    "                entities.append(token[\"kata\"])\n",
    "                \n",
    "    return entities\n",
    "\n",
    "def get_content_word_np(row):\n",
    "    entities = []\n",
    "    for token in row[\"question_features\"][0]:\n",
    "        if \"NP\" in token[\"chunk\"] and token[\"pos\"] != \"B-WH\" and token[\"kata\"].lower() not in eat_dict_4.keys() and token[\"ner\"] == \"O\":\n",
    "            entities.extend(preprocess_text(token[\"kata\"]))\n",
    "    return entities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0d845-2529-46b1-9218-8dac5d1fabc4",
   "metadata": {},
   "source": [
    "## Filter Answer By Content Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ed631bf-d0b3-4f81-86ff-19591bcf4008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_by_content_word(row, key_context, key_content_word, key_candidate_ans):\n",
    "    sentences = row[key_context]\n",
    "    candidate_ans_filtered = {}\n",
    "    idx_sentences_filtered = []\n",
    "    entities = row[key_content_word]\n",
    "    \n",
    "    if key_context == \"context_coref\":\n",
    "        sentences = split_sentences(row[key_context])\n",
    "        \n",
    "    if not(len(entities)):\n",
    "        return {}\n",
    "    \n",
    "    if key_context == \"context_coref\":\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            is_all = True\n",
    "            for i in entities:\n",
    "                if remove_punctuation(i.lower()).strip() not in sentence.lower().strip():\n",
    "                    is_all = False\n",
    "                    break\n",
    "            if is_all:\n",
    "                idx_sentences_filtered.append(idx)\n",
    "\n",
    "        for k in row[key_candidate_ans].keys():\n",
    "            if k[0] in idx_sentences_filtered:\n",
    "                candidate_ans_filtered[k] = row[key_candidate_ans][k]\n",
    "    else:\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            is_all = True\n",
    "            for i in entities:\n",
    "                if remove_punctuation(i.lower()).strip() not in remove_punctuation(\" \".join(sentence).lower()).strip():\n",
    "                    is_all = False\n",
    "                    break\n",
    "            if is_all:\n",
    "                idx_sentences_filtered.append(idx)\n",
    "\n",
    "        try:\n",
    "            for k in row[key_candidate_ans].keys():\n",
    "                if k[0] in idx_sentences_filtered:\n",
    "                    candidate_ans_filtered[k] = row[key_candidate_ans][k]\n",
    "        except:\n",
    "            return {}\n",
    "            \n",
    "    return candidate_ans_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9085453-e677-4052-bc34-f4602538d500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_by_content_word_c(row, key_context, key_content_word, key_candidate_ans):\n",
    "    sentences = row[key_context]\n",
    "    candidate_ans_filtered = {}\n",
    "    idx_sentences_filtered = []\n",
    "    entities = row[key_content_word]\n",
    "    \n",
    "    if key_context == \"context_coref\":\n",
    "        sentences = split_sentences(row[key_context])\n",
    "        \n",
    "    if not(len(entities)):\n",
    "        return {}\n",
    "    \n",
    "    if key_context == \"context_coref\":\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            is_all = True\n",
    "            for i in entities:\n",
    "                if remove_punctuation(i.lower()).strip() not in sentence.lower().strip():\n",
    "                    is_all = False\n",
    "                    break\n",
    "            if is_all:\n",
    "                idx_sentences_filtered.append(idx)\n",
    "\n",
    "        for k in row[key_candidate_ans].keys():\n",
    "            if k[0] in idx_sentences_filtered:\n",
    "                candidate_ans_filtered[k] = row[key_candidate_ans][k]\n",
    "    else:\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            is_all = True\n",
    "            for i in entities:\n",
    "                if remove_punctuation(i.lower()).strip() not in remove_punctuation(\" \".join(sentence).lower()).strip():\n",
    "                    is_all = False\n",
    "                    break\n",
    "            if is_all:\n",
    "                idx_sentences_filtered.append(idx)\n",
    "\n",
    "        try:\n",
    "            for k in row[key_candidate_ans].keys():\n",
    "                if k[0] in idx_sentences_filtered:\n",
    "                    candidate_ans_filtered[k] = row[key_candidate_ans][k]\n",
    "        except:\n",
    "            return {}\n",
    "            \n",
    "    return candidate_ans_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e675a75-f903-4781-9277-5e005dff0ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_by_content_word_or(row, key_context, key_content_word, key_candidate_ans):\n",
    "    sentences = row[key_context]\n",
    "    candidate_ans_filtered = {}\n",
    "    idx_sentences_filtered = []\n",
    "    entities = row[key_content_word]\n",
    "    \n",
    "    if key_context == \"context_coref\":\n",
    "        sentences = split_sentences(row[key_context])\n",
    "        \n",
    "    if not(len(entities)):\n",
    "        return {}\n",
    "    \n",
    "    if key_context == \"context_coref\":\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            is_any = False\n",
    "            for i in entities:\n",
    "                for word in sentence:\n",
    "                    if remove_punctuation(i.lower()).strip() in sentence.lower().strip():\n",
    "                        is_any = True\n",
    "                        break\n",
    "            if not len(entities):\n",
    "                is_any = True\n",
    "            if is_any:\n",
    "                idx_sentences_filtered.append(idx)\n",
    "\n",
    "        for k in row[key_candidate_ans].keys():\n",
    "            if k[0] in idx_sentences_filtered:\n",
    "                candidate_ans_filtered[k] = row[key_candidate_ans][k]\n",
    "    else:\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            is_any = False\n",
    "            for i in entities:\n",
    "                for word in sentence:\n",
    "                    if remove_punctuation(i.lower()).strip() in remove_punctuation(\" \".join(sentence).lower()).strip():\n",
    "                        is_any = True\n",
    "                        break\n",
    "\n",
    "            if not len(entities):\n",
    "                is_any = True\n",
    "            if is_any:\n",
    "                idx_sentences_filtered.append(idx)\n",
    "        try:\n",
    "            for k in row[key_candidate_ans].keys():\n",
    "                if k[0] in idx_sentences_filtered:\n",
    "                    candidate_ans_filtered[k] = row[key_candidate_ans][k]\n",
    "        except:\n",
    "            return {}\n",
    "            \n",
    "    return candidate_ans_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5725d-5904-42c2-9bcc-e1a4bb471c4b",
   "metadata": {},
   "source": [
    "## Get Passage Feature (*get_feature*(passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c97f1adb-6be7-42a1-be56-263924febc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature(passage):\n",
    "    passage_sentences = split_sentences(passage)\n",
    "    sentences_ner = get_tag_predictions(modul_ner, tokenizer_ner, passage_sentences)\n",
    "    sentences_pos = get_tag_predictions(modul_pos, tokenizer_pos, passage_sentences)\n",
    "    sentences_chunk = get_tag_predictions(modul_chunking, tokenizer_chunking, passage_sentences)\n",
    "    sentences_timex = get_tag_predictions(modul_timex, tokenizer_timex, passage_sentences)\n",
    "    passage = []\n",
    "    for i in range(len(sentences_ner)): #List of sentence\n",
    "        sentence = []\n",
    "        for j in range(len(sentences_ner[i])): #List of token\n",
    "            sentence.append({\n",
    "                \"kata\": sentences_ner[i][j][0],\n",
    "                \"ner\": sentences_ner[i][j][1],\n",
    "                \"pos\": sentences_pos[i][j][1],\n",
    "                \"chunk\": sentences_chunk[i][j][1],\n",
    "                \"timex\": sentences_timex[i][j][1],\n",
    "            })\n",
    "        passage.append(sentence)\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b705cb-5273-4aa3-a5d7-eb81e2fb9328",
   "metadata": {},
   "source": [
    "# Eval Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcbba6af-3d1a-427b-9750-4e0bc27cf111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "  import string, re\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    return re.sub(regex, \" \", text)\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    try:\n",
    "        return text.lower()\n",
    "    except:\n",
    "        print(type(text))\n",
    "        print(text)\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match(prediction, truth):\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "  pred_tokens = normalize_text(prediction).split()\n",
    "  truth_tokens = normalize_text(truth).split()\n",
    "  \n",
    "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "    return int(pred_tokens == truth_tokens)\n",
    "  \n",
    "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "  \n",
    "  # if there are no common tokens then f1 = 0\n",
    "  if len(common_tokens) == 0:\n",
    "    return 0\n",
    "  \n",
    "  prec = len(common_tokens) / len(pred_tokens)\n",
    "  rec = len(common_tokens) / len(truth_tokens)\n",
    "  \n",
    "  return round(2 * (prec * rec) / (prec + rec), 2)\n",
    "\n",
    "def eval_all(df, pred_key):\n",
    "    total_exact = 0\n",
    "    total_ans = 0\n",
    "    total_unans = 0\n",
    "    total_f1 = 0\n",
    "    total_f1_ans = 0\n",
    "    total_exact_unans = 0\n",
    "    total_exact_ans = 0\n",
    "    try:\n",
    "        ground_truth = [i[\"text\"] for i in df[\"answer\"]]\n",
    "    except:\n",
    "        ground_truth = df[\"answer\"]\n",
    "    total_question = len(ground_truth)\n",
    "    \n",
    "    pred = df[pred_key]\n",
    "    for p,t in zip(pred, ground_truth):\n",
    "        try:\n",
    "            len(t)\n",
    "        except:\n",
    "            t = \"\"\n",
    "        if not len(t):\n",
    "            total_unans += 1\n",
    "            total_exact_unans += exact_match(p,t)\n",
    "        else:\n",
    "            total_ans +=1 \n",
    "            total_exact_ans += exact_match(p,t)\n",
    "            total_f1_ans += compute_f1(p,t)\n",
    "            \n",
    "        \n",
    "        total_f1 += compute_f1(p,t)\n",
    "        total_exact += exact_match(p,t)\n",
    "\n",
    "    print(f\"Exact match all: {total_exact/len(ground_truth)}\")\n",
    "    print(f\"F1 all: {total_f1/len(ground_truth)}\")\n",
    "    print(f\"Exact match answerable: {total_exact_ans/total_ans}\")\n",
    "    print(f\"F1 answerable: {total_f1_ans/total_ans}\")\n",
    "    if total_unans:\n",
    "        print(f\"Exact match unanswerebale: {total_exact_unans/total_unans}\")\n",
    "    else:\n",
    "        print(\"No unnswerable question\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38539c3e-203f-40cd-9adc-846bc3b378ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_candidate_answers(df, key):\n",
    "    total_exact = 0\n",
    "    total_ans = 0\n",
    "    total_unans = 0\n",
    "    total_f1 = 0\n",
    "    total_f1_ans = 0\n",
    "    total_exact_unans = 0\n",
    "    total_exact_ans = 0\n",
    "    f1s = []\n",
    "    try:\n",
    "        ground_truth = [i[\"text\"] for i in df[\"answer\"]]\n",
    "    except:\n",
    "        ground_truth = df[\"answer\"]\n",
    "        \n",
    "    total_question = len(ground_truth)\n",
    "    \n",
    "    preds = df[key]\n",
    "    \n",
    "    for p,t in zip(preds, ground_truth):\n",
    "        if not len(t):\n",
    "            total_unans += 1\n",
    "        else:\n",
    "            total_ans += 1\n",
    "        \n",
    "        \n",
    "        p[(-1, -1, -1)] = \"\"\n",
    "            \n",
    "        for key in p:\n",
    "            f1_now = compute_f1(p[key], t)\n",
    "            if f1_now > max_f1:\n",
    "                max_f1 = f1_now\n",
    "        \n",
    "        if max_f1 == 1:\n",
    "            if not len(t):\n",
    "                total_exact_unans += 1\n",
    "            else:\n",
    "                total_exact_ans += 1\n",
    "                total_f1_ans += 1\n",
    "            total_exact += 1\n",
    "        else:\n",
    "            total_f1_ans += max_f1\n",
    "        \n",
    "        f1s.append(max_f1)\n",
    "        total_f1 += max_f1\n",
    "\n",
    "    print(f\"Exact match all: {total_exact/len(ground_truth)}\")\n",
    "    print(f\"F1 all: {total_f1/len(ground_truth)}\")\n",
    "    print(f\"Exact match answerable: {total_exact_ans/total_ans}\")\n",
    "    print(f\"F1 answerable: {total_f1_ans/total_ans}\")\n",
    "    if total_unans:\n",
    "        print(f\"Exact match unanswerebale: {total_exact_unans/total_unans}\")\n",
    "    else:\n",
    "        print(\"No unnswerable question\")\n",
    "    print(\"Total Question: \", len(ground_truth))\n",
    "    print(\"Total Ans Question: \", total_ans)     \n",
    "    print(\"Total Unans Question: \", total_unans)\n",
    "    \n",
    "    return f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccace6-18c9-4d8e-91da-b98f38e91f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Eksperimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8203f4dd-e480-499f-b458-d0c255016d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17324617-9935-4fc8-a8f5-2d0baaef03e8",
   "metadata": {},
   "source": [
    "## Get Feature Passage and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3eb37364-4ceb-4b0c-8879-d8a22ddc5915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_all(df):\n",
    "    df[\"question_features\"] = df[\"question\"].progress_apply(lambda x: get_feature(x))\n",
    "    try:\n",
    "        df[\"context_features\"] =  df[\"context\"].progress_apply(lambda x: get_feature(x))\n",
    "    except:\n",
    "        df[\"context_features\"] =  df[\"passage\"].progress_apply(lambda x: get_feature(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa2af1-84e3-406e-9f11-8efe571bae0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_feature_all(tydiqa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f83d37-65ac-4d46-95db-0e8ad4aed675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_feature_all(idkmrc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_all(squad_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd9b7-58c8-4612-a716-061721a32078",
   "metadata": {},
   "source": [
    "## Get Kata Tanya dan EAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3e5a81c-43c1-4ffc-b08e-a3005a5e38f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kata_tanya_eat(df):\n",
    "    # NER + TIMEX\n",
    "    df[['closed_question_1', 'eat_1']] = df['question'].apply(lambda x: pd.Series(get_eat(x, eat_dict_2)))\n",
    "    # NER + TIMEX + POS\n",
    "    df[['closed_question_2', 'eat_2']] = df['question'].apply(lambda x: pd.Series(get_eat(x, eat_dict_3)))    \n",
    "    # Chunk + POS\n",
    "    df[['open_question', 'eat_3']] = df['question'].apply(lambda x: pd.Series(get_eat(x, eat_dict_4))) \n",
    "    # All\n",
    "    df[['all_question', 'eat_4']] = df['question'].apply(lambda x: pd.Series(get_eat(x, eat_dict_5)))\n",
    "    # \n",
    "    df[['closed_question_3', 'eat_5']] = df['question'].apply(lambda x: pd.Series(get_eat(x, eat_dict_6)))     \n",
    "    # All old\n",
    "    df[['all_question_2', 'eat_6']] = df['question'].apply(lambda x: pd.Series(get_eat(x, eat_dict)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "780d0ae8-fc94-4521-9745-68e0d177f791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_kata_tanya_eat(squad_test)\n",
    "get_kata_tanya_eat(tydiqa_test)\n",
    "get_kata_tanya_eat(idkmrc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b903e4a-b8d9-4d9c-b87b-42744393fa7c",
   "metadata": {},
   "source": [
    "## Get Candidate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0052bd6d-c66b-4215-9eaa-69cc02ff8a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_candidate_ans(df):\n",
    "    df[\"candidate_answers_1\"] = df.progress_apply(lambda row: get_candidate_answer_2(row[\"eat_1\"], row[\"context_features\"]) if row[\"eat_1\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_1\"] = df.progress_apply(lambda row: get_final_candidate_answers_2(row[\"candidate_answers_1\"], row[\"context_features\"]) if row[\"eat_1\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_2\"] = df.progress_apply(lambda row: get_candidate_answer_2(row[\"eat_2\"], row[\"context_features\"]) if row[\"eat_2\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_2\"] = df.progress_apply(lambda row: get_final_candidate_answers_2(row[\"candidate_answers_2\"], row[\"context_features\"]) if row[\"eat_2\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_3\"] = df.progress_apply(lambda row: get_candidate_answer_2(row[\"eat_3\"], row[\"context_features\"]) if row[\"eat_3\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_3\"] = df.progress_apply(lambda row: get_final_candidate_answers_2(row[\"candidate_answers_3\"], row[\"context_features\"]) if row[\"eat_3\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_4\"] = df.progress_apply(lambda row: get_candidate_answer_2(row[\"eat_4\"], row[\"context_features\"]) if row[\"eat_4\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_4\"] = df.progress_apply(lambda row: get_final_candidate_answers_2(row[\"candidate_answers_4\"], row[\"context_features\"]) if row[\"eat_4\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_5\"] = df.progress_apply(lambda row: get_candidate_answer_2(row[\"eat_5\"], row[\"context_features\"]) if row[\"eat_5\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_5\"] = df.progress_apply(lambda row: get_final_candidate_answers_2(row[\"candidate_answers_5\"], row[\"context_features\"]) if row[\"eat_5\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_6\"] = df.progress_apply(lambda row: get_candidate_answer_2(row[\"eat_6\"], row[\"context_features\"]) if row[\"eat_6\"] else {}, axis=1)\n",
    "    df[\"candidate_answers_6\"] = df.progress_apply(lambda row: get_final_candidate_answers_2(row[\"candidate_answers_6\"], row[\"context_features\"]) if row[\"eat_6\"] else {}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303e8e2-fe2d-4f88-b629-d4fec6a2b9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_candidate_ans(squad_test)\n",
    "get_candidate_ans(tydiqa_test)\n",
    "get_candidate_ans(idkmrc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d03893-415e-4776-bafa-409e02d8bb37",
   "metadata": {},
   "source": [
    "## Eval Candidate Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6212597-72a3-4f67-bae0-38f4c6318917",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idkmrc_test[\"f1_1\"] = eval_candidate_answers(idkmrc_test, \"candidate_answers_1\" ) \n",
    "idkmrc_test[\"f1_2\"] = eval_candidate_answers(idkmrc_test, \"candidate_answers_2\" ) \n",
    "idkmrc_test[\"f1_3\"] = eval_candidate_answers(idkmrc_test, \"candidate_answers_3\" ) \n",
    "idkmrc_test[\"f1_4\"] = eval_candidate_answers(idkmrc_test, \"candidate_answers_4\" ) \n",
    "idkmrc_test[\"f1_5\"] = eval_candidate_answers(idkmrc_test, \"candidate_answers_5\" ) \n",
    "\n",
    "squad_test[\"f1_1\"] = eval_candidate_answers(squad_test, \"candidate_answers_1\" ) \n",
    "squad_test[\"f1_2\"] = eval_candidate_answers(squad_test, \"candidate_answers_2\" ) \n",
    "squad_test[\"f1_3\"] = eval_candidate_answers(squad_test, \"candidate_answers_3\" ) \n",
    "squad_test[\"f1_4\"] = eval_candidate_answers(squad_test, \"candidate_answers_4\" ) \n",
    "squad_test[\"f1_5\"] = eval_candidate_answers(squad_test, \"candidate_answers_5\" ) \n",
    "\n",
    "tydiqa_test[\"f1_1\"] = eval_candidate_answers(tydiqa_test, \"candidate_answers_1\" ) \n",
    "tydiqa_test[\"f1_2\"] = eval_candidate_answers(tydiqa_test, \"candidate_answers_2\" ) \n",
    "tydiqa_test[\"f1_3\"] = eval_candidate_answers(tydiqa_test, \"candidate_answers_3\" ) \n",
    "tydiqa_test[\"f1_4\"] = eval_candidate_answers(tydiqa_test, \"candidate_answers_4\" ) \n",
    "tydiqa_test[\"f1_5\"] = eval_candidate_answers(tydiqa_test, \"candidate_answers_5\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc107b8b-8597-4564-b9d4-62186b786dfe",
   "metadata": {},
   "source": [
    "## Ekstrak Content Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3f4293a-c401-45ce-bd30-14da87729c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_list(lst):\n",
    "    temp = []\n",
    "    for i in lst:\n",
    "        temp.extend(preprocess_text(i)[0:1])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bce80e6-95bb-4b66-b040-7e1487fa6e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preprocessed(row, key_feature):\n",
    "    sentences = row[key_feature]\n",
    "    temp = []\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        s = []\n",
    "        for word in sentence:\n",
    "            if \"NP\" in word[\"chunk\"] and word[\"ner\"] == \"O\":\n",
    "                s.extend(preprocess_text(word[\"kata\"])[0:1])\n",
    "            else:\n",
    "                s.append(word[\"kata\"])\n",
    "        temp.append(s)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cb658f8-723f-459d-b1f5-260c940a7148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_df(df):\n",
    "    df[\"lemma\"] = df.progress_apply(lambda row: get_preprocessed(row, \"context_features\"), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c9b9d-d24f-4bde-a53e-48cb4ac5b501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_preprocessed_df(squad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f164cf-3621-4aac-acd9-1f5fa73d4014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_preprocessed_df(tydiqa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d1b23-46ba-489c-a8d6-3f1b39ecf588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_preprocessed_df(idkmrc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "325a629a-e0c1-4cd5-8269-0c4dd30b0e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_content_entities(df):\n",
    "    df[\"content_entities\"] = df.progress_apply(lambda row: get_content_word_entity(row), axis=1)\n",
    "    df[\"content_np\"] = df.progress_apply(lambda row: get_content_word_np(row), axis=1)\n",
    "    df[\"content_entities_np\"] = df[\"content_entities\"] + df[\"content_np\"]\n",
    "    df[\"filtered_candidate_answer_entities\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"lemma\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_np\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"lemma\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_or\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"lemma\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_np_or\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"lemma\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_np\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"lemma\", \"content_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_np_or\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"lemma\", \"content_np\", \"candidate_answers_4\"), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "307b5d8d-ff60-4bbf-acb6-5f499d201f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_content_entities_2(df):\n",
    "    df[\"filtered_candidate_answer_entities_2\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_np_2\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_or_2\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"context_coref\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_np_or_2\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"context_coref\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_np_2\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_np_or_2\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"context_coref\", \"content_np\", \"candidate_answers_4\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a212175-5e40-4a6d-acc4-1fe36c80a9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filter_acc(df, key_1, key_2):\n",
    "    total = 0\n",
    "    for i, j in zip(df[key_1], df[key_2]):\n",
    "        if j >= i:\n",
    "            total += 1\n",
    "            \n",
    "    print(total / len(df[key_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c70f4a-2e34-4e50-898f-71fb8b0b90f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_content_entities(squad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f9338-a68a-47d7-bd42-fe9535b6c576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_content_entities(tydiqa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e34d73-9f9d-4dbb-844a-ca5aa7eaf4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_content_entities(idkmrc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9dd91408-e2d1-4d43-a129-cae7dc878da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coreferential_case(df):\n",
    "    filtered_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        kalimat = split_sentences(row[\"context\"])\n",
    "        # print(kalimat)\n",
    "        kata_entitas = row[\"content_entities\"]   \n",
    "        a = row[\"answer\"]\n",
    "        if a[\"text\"] == \"\":\n",
    "            continue\n",
    "        if not len(kata_entitas):\n",
    "            continue\n",
    "        # Menghitung panjang karakter untuk setiap kalimat\n",
    "        panjang_kalimat = [len(k)+1 for k in kalimat]\n",
    "\n",
    "        # Menentukan indeks kalimat berdasarkan indeks karakter yang diberikan\n",
    "        total_karakter = 0\n",
    "        \n",
    "        s = None\n",
    "        for panjang, k in zip(panjang_kalimat, kalimat):\n",
    "            total_karakter += panjang\n",
    "            if a[\"answer_start\"] < total_karakter and a[\"text\"] in k :\n",
    "                s = k\n",
    "                # print(q, k)\n",
    "        if s == None:\n",
    "            continue\n",
    "        is_ada_satu = False\n",
    "        for i in kata_entitas:\n",
    "            # print(i.lower(), s.lower())\n",
    "            \n",
    "            if remove_punctuation(i.lower()) in remove_punctuation(s.lower()):\n",
    "                is_ada_satu = True\n",
    "                \n",
    "        if not is_ada_satu:\n",
    "            # print(\"Masuk\")\n",
    "            filtered_rows.append(row)\n",
    "            \n",
    "    return pd.DataFrame(filtered_rows)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "idkmrc_test[\"f1_t_1\"] = eval_candidate_answers(idkmrc_test, \"filtered_candidate_answer_entities\")\n",
    "idkmrc_test[\"f1_t_2\"] = eval_candidate_answers(idkmrc_test, \"filtered_candidate_answer_entities_or\")\n",
    "idkmrc_test[\"f1_t_3\"] = eval_candidate_answers(idkmrc_test, \"filtered_candidate_answer_np\")\n",
    "idkmrc_test[\"f1_t_4\"] = eval_candidate_answers(idkmrc_test, \"filtered_candidate_answer_np_or\")\n",
    "idkmrc_test[\"f1_t_5\"] = eval_candidate_answers(idkmrc_test, \"filtered_candidate_answer_entities_np\")\n",
    "idkmrc_test[\"f1_t_6\"] = eval_candidate_answers(idkmrc_test, \"filtered_candidate_answer_entities_np_or\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_test[\"f1_t_1\"] = eval_candidate_answers(squad_test, \"filtered_candidate_answer_entities\")\n",
    "squad_test[\"f1_t_2\"] = eval_candidate_answers(squad_test, \"filtered_candidate_answer_entities_or\")\n",
    "squad_test[\"f1_t_3\"] = eval_candidate_answers(squad_test, \"filtered_candidate_answer_np\")\n",
    "squad_test[\"f1_t_4\"] = eval_candidate_answers(squad_test, \"filtered_candidate_answer_np_or\")\n",
    "squad_test[\"f1_t_5\"] = eval_candidate_answers(squad_test, \"filtered_candidate_answer_entities_np\")\n",
    "squad_test[\"f1_t_6\"] = eval_candidate_answers(squad_test, \"filtered_candidate_answer_entities_np_or\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d77028",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_test[\"f1_t_1\"] = eval_candidate_answers(tydiqa_test, \"filtered_candidate_answer_entities\")\n",
    "tydiqa_test[\"f1_t_2\"] = eval_candidate_answers(tydiqa_test, \"filtered_candidate_answer_entities_or\")\n",
    "tydiqa_test[\"f1_t_3\"] = eval_candidate_answers(tydiqa_test, \"filtered_candidate_answer_np\")\n",
    "tydiqa_test[\"f1_t_4\"] = eval_candidate_answers(tydiqa_test, \"filtered_candidate_answer_np_or\")\n",
    "tydiqa_test[\"f1_t_5\"] = eval_candidate_answers(tydiqa_test, \"filtered_candidate_answer_entities_np\")\n",
    "tydiqa_test[\"f1_t_6\"] = eval_candidate_answers(tydiqa_test, \"filtered_candidate_answer_entities_np_or\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c54ec-159d-48ab-8d88-921f0ca4b568",
   "metadata": {},
   "source": [
    "## Get Passage Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fa50fa55-a9a1-4304-87fe-927b3308a92b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_passage_coref(df):\n",
    "    df[\"context_coref\"] = df.progress_apply(lambda row: replace_coref(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4968d-4afa-4056-bb07-4b0b1350bb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_passage_coref(idkmrc_test)\n",
    "get_passage_coref(tydiqa_test)\n",
    "get_passage_coref(squad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a4f5798c-f5f4-4a68-a71c-97d164d7d004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_content_entities_after_coref(df):\n",
    "    df[\"filtered_candidate_answer_entities_coref\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_np_coref\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_or_coref\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"context_coref\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_entities_np_or_coref\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"context_coref\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_np_coref\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_np\", \"candidate_answers_4\"), axis=1)\n",
    "    df[\"filtered_candidate_answer_np_or_coref\"] = df.progress_apply(lambda row: filter_by_content_word_or(row, \"context_coref\", \"content_np\", \"candidate_answers_4\"), axis=1)\n",
    "    \n",
    "#     df[\"filtered_candidate_answer_entities_coref\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_entities\", \"candidate_answers_4\"), axis=1)\n",
    "#     df[\"filtered_candidate_answer_entities_np_coref\"] = df.progress_apply(lambda row: filter_by_content_word(row, \"context_coref\", \"content_entities_np\", \"candidate_answers_4\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638010f6-5307-4313-83f4-5d2b980283a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_content_entities_after_coref(idkmrc_test)\n",
    "get_content_entities_after_coref(tydiqa_test)\n",
    "get_content_entities_after_coref(squad_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52e1d1-daf5-4ea6-ac65-05b272d0bdba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "436be825-5141-41d7-8300-116284be1b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dependency_graph(row):\n",
    "    sentences = [[word[\"kata\"] for word in sentence] for sentence in row[\"context_features\"] ]\n",
    "    pos_tags = [[word[\"pos\"] for word in sentence] for sentence in row[\"context_features\"] ]\n",
    "    deps = []\n",
    "    idx_sentence_candidate = set([k[0] for k in row[\"filtered_candidate_answer_entities_np_or\"].keys()])\n",
    "    \n",
    "    count = 0\n",
    "    for s, t in zip(sentences, pos_tags):\n",
    "        if count not in idx_sentence_candidate:\n",
    "            deps.append([])\n",
    "        else:\n",
    "            deps.append(predict_dependency(s, t))\n",
    "        count += 1\n",
    "    \n",
    "    return deps\n",
    "\n",
    "def get_df_dep(df):\n",
    "    df[\"dependencies\"] = df.progress_apply(lambda row: get_dependency_graph(row), axis =1)\n",
    "    \n",
    "def get_dependency_graph_q(row):\n",
    "    sentences = [[]]\n",
    "    pos_tags = [[]]\n",
    "    for sentence in row[\"question_features\"]:\n",
    "        for word in sentence:\n",
    "            # print(word)\n",
    "            if \"WH\" not in word[\"pos\"] and word[\"kata\"].lower() not in eat_dict_4.keys() and \"Z\" not in word[\"pos\"]:\n",
    "                sentences[0].append(word[\"kata\"])\n",
    "                pos_tags[0].append(word[\"pos\"])\n",
    "    deps = predict_dependency(sentences[0], pos_tags[0])\n",
    "    \n",
    "    return deps\n",
    "\n",
    "def get_df_dep_q(df):\n",
    "    df[\"q_dependencies\"] = df.progress_apply(lambda row: get_dependency_graph_q(row), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "7e86b92f-9de3-4ca6-8cb1-82e362d95afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dependency_graph_2(row):\n",
    "    sentences = [[word[\"kata\"] for word in sentence] for sentence in row[\"context_features\"] ]\n",
    "    pos_tags = [[word[\"pos\"] for word in sentence] for sentence in row[\"context_features\"] ]\n",
    "    deps = row[\"dependencies\"]\n",
    "    idx_sentence_candidate = set([k[0] for k in row[\"filtered_candidate_answer_entities_np_or\"].keys()])\n",
    "    # print(idx_sentence_candidate)\n",
    "    count = 0\n",
    "    for s, t, d in zip(sentences, pos_tags, deps):\n",
    "        # print(d)\n",
    "        if len(d) != 0:\n",
    "            # print(count, \"Masuk\")\n",
    "            count += 1 \n",
    "            continue\n",
    "            \n",
    "        if count not in idx_sentence_candidate:\n",
    "            # print(\"Lahhh\", count)\n",
    "            deps.append([])\n",
    "        else:\n",
    "            deps[count] = predict_dependency(s, t)\n",
    "        count += 1\n",
    "    \n",
    "    return deps\n",
    "\n",
    "def get_df_dep_2(df):\n",
    "    df[\"dependencies\"] = df.progress_apply(lambda row: get_dependency_graph_2(row), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d66de1-fd82-4870-b0fe-ab4cf631d369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_df_dep(squad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80fcd5-05a2-4049-b439-1484a9039d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_df_dep(tydiqa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3cb14-833c-4a98-9eb5-8a647e2cc9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_df_dep(idkmrc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e525b-79ee-4d10-ba03-90136aed45e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_df_dep_q(squad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231ab62-35b5-4f01-b31a-2581c9dd888a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_df_dep_q(tydiqa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60b289-119e-4266-a194-f3f391c80095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_df_dep_q(idkmrc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53898b50-2983-4cb6-b7ae-1dcb71618100",
   "metadata": {},
   "source": [
    "## Built Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c77f9b14-d887-48e4-abf7-022930aad048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_numbers_regex(string):\n",
    "    pattern = r'\\d+'  # Matches one or more digits\n",
    "    return re.sub(pattern, '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "51626b59-9728-456e-8e01-e26e83fd5f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def create_directed_graph(nodes):\n",
    "    graph = nx.DiGraph()\n",
    "    for node in nodes:\n",
    "        index, name, label, parent = node\n",
    "        graph.add_node(index, name=name)\n",
    "        if parent != -1:\n",
    "            graph.add_edge(parent, index, label=label)\n",
    "            \n",
    "    return graph\n",
    "\n",
    "def create_undirected_graph(nodes):\n",
    "    graph = nx.Graph()\n",
    "    for node in nodes:\n",
    "        index, name, label, parent = node\n",
    "        graph.add_node(index, name=name)\n",
    "        if parent != -1:\n",
    "            graph.add_edge(parent, index, label=label)\n",
    "    return graph\n",
    "\n",
    "def calculate_distance(graph, source, target):\n",
    "    try:\n",
    "        return nx.shortest_path_length(graph, source, target)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float('9999')\n",
    "    \n",
    "# def get_target_idx(row, key_candidate, key_content_word):\n",
    "#     idxs = {}\n",
    "#     for k in row[key_candidate].keys():\n",
    "#         if k[0] in idxs.keys():\n",
    "#             continue\n",
    "#         for idx, word in enumerate(row[\"context_features\"][k[0]]):\n",
    "#             for target in row[key_content_word]:\n",
    "#                 if remove_punctuation(word[\"kata\"]).lower().strip().maketrans('', '', string.digits) == remove_punctuation(target).lower().strip().maketrans('', '', string.digits):\n",
    "#                     idxs.setdefault(k[0], []).append(idx)\n",
    "                    \n",
    "#     return idxs\n",
    "\n",
    "def get_target_idx(row, key_candidate, key_content_word):\n",
    "    idxs = {}\n",
    "    for k in row[key_candidate].keys():\n",
    "        if k[0] in idxs.keys() or k[0] < 0:\n",
    "            continue\n",
    "        for idx, word in enumerate(row[\"lemma\"][k[0]]):\n",
    "            for target in row[key_content_word]:\n",
    "                if remove_numbers_regex(remove_punctuation(word).lower().strip()) == remove_numbers_regex(remove_punctuation(target).lower().strip()):\n",
    "                    # print(word, target)\n",
    "                    idxs.setdefault(k[0], []).append(idx)\n",
    "                    \n",
    "    return idxs\n",
    "\n",
    "def get_target_idx_2(row, key_candidate, key_content_word, kata):\n",
    "    idxs = {}\n",
    "    for k in row[key_candidate].keys():\n",
    "        if k[0] in idxs.keys() or k[0] < 0:\n",
    "            continue\n",
    "        for idx, word in enumerate(row[\"lemma\"][k[0]]):\n",
    "            try:\n",
    "                if remove_numbers_regex(remove_punctuation(word).lower().strip()) == remove_numbers_regex(remove_punctuation(preprocess_text(kata)[0]).lower().strip()):\n",
    "                    # print(word, target)\n",
    "                    idxs.setdefault(k[0], []).append(idx)\n",
    "            except IndexError:\n",
    "                if remove_numbers_regex(remove_punctuation(word).lower().strip()) == remove_numbers_regex(remove_punctuation(kata).lower().strip()):\n",
    "                    # print(word, target)\n",
    "                    idxs.setdefault(k[0], []).append(idx)\n",
    "                    \n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "827f4ce7-d647-4cff-adc3-f628f270b931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_directed_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    labels = {node: f\"{node}: {data['name']}\" for node, data in graph.nodes(data=True)}\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw_networkx(graph, pos, with_labels=True, labels=labels, node_color='lightblue',\n",
    "                     node_size=800, font_size=10, arrows=True, arrowstyle='->')\n",
    "    plt.title(\"Directed Graph Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_undirected_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    labels = {node: f\"{node}: {data['name']}\" for node, data in graph.nodes(data=True)}\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw_networkx(graph, pos, with_labels=True, labels=labels, node_color='lightblue',\n",
    "                     node_size=800, font_size=10, edge_color='gray', linewidths=0.5)\n",
    "    plt.title(\"Undirected Graph Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "a7db068c-57bc-496d-b60c-e9153d57a563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lowest_key(my_dict):\n",
    "    min_value = min(my_dict.values())\n",
    "    keys_with_lowest_value = [key for key, value in my_dict.items() if value == min_value]\n",
    "    return keys_with_lowest_value\n",
    "\n",
    "def get_answer(row, key_candidate, key_word_content):\n",
    "    targets = get_target_idx(row, key_candidate, key_word_content)\n",
    "    directed_graphs = {}\n",
    "    undirected_graphs = {}\n",
    "    filter_by_label = {}\n",
    "    if len(row[key_candidate]) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # print(row[key_candidate])\n",
    "    for k, v in row[key_candidate].items():\n",
    "        directed_graphs[k[0]] = create_directed_graph(row[\"dependencies\"][k[0]])\n",
    "        undirected_graphs[k[0]] = create_undirected_graph(row[\"dependencies\"][k[0]])\n",
    "    \n",
    "    for k, v in row[key_candidate].items():\n",
    "        # print(v)\n",
    "        for i, j in enumerate(v.split()):\n",
    "            node_index = k[1] + i \n",
    "            try:\n",
    "                parent_index = list(directed_graphs[k[0]].predecessors(node_index))[0]\n",
    "                label = directed_graphs[k[0]].get_edge_data(parent_index, node_index)['label']\n",
    "            except IndexError:\n",
    "                label = \"root\"\n",
    "            # except nx.exception.NetworkXError:\n",
    "            #     print(row[\"context\"])\n",
    "            \n",
    "            # print(label)\n",
    "            if row[\"all_question\"] in [\"apa\", \"apakah\"]:\n",
    "                if label in [\"nsubj\", \"root\", \"obj\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            elif row[\"all_question\"] in [\"kapan\", \"kapankah\"]:\n",
    "                if label in [\"nmod\", \"nummod\", \"appos\", \"root\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            elif row[\"all_question\"] in [\"dimana\", \"di mana\", \"darimanakah\", \"dari manakah\", \"darimana\", \"dari mana\", \"dimanakah\", \"di manakah\", \"manakah\", \"mana\", \"kemana\", \"ke mana\", \"kemanakah\", \"ke manakah\"]:\n",
    "                if label in [\"nmod\", \"obl\", \"root\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            elif row[\"all_question\"] in [\"siapa\", \"siapakah\"]:\n",
    "                if label in [\"flat\", \"root\", \"nsubj\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            elif row[\"all_question\"] in [\"berapa\", \"berapakah\", \"seberapa\"]:\n",
    "                if label in [\"nmod\", \"root\", \"nummod\", \"punct\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            elif row[\"all_question\"] in [\"kenapa\", \"mengapa\", \"beberapa\", \"bagaimanakah\"]:\n",
    "                if label in [\"compound\", \"obj\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            elif row[\"all_question\"] in [\"bagaimana\"]:\n",
    "                if label in [\"compound\", \"obj\", \"root\"]:\n",
    "                    filter_by_label[k] = v\n",
    "            else:\n",
    "                filter_by_label[k] = v\n",
    "                    \n",
    "    return filter_by_label\n",
    "    # print(filter_by_label)\n",
    "    dis_directed = {}\n",
    "    dis_undirected = {}\n",
    "    for k, v in filter_by_label.items():\n",
    "        found_overlap = True\n",
    "        for en in row[key_word_content]:\n",
    "            if remove_punctuation(en).lower().strip() not in v.lower():\n",
    "                found_overlap = False\n",
    "                break\n",
    "        if found_overlap:\n",
    "            continue\n",
    "        total_1 = 0\n",
    "        total_2 = 0\n",
    "        # try:\n",
    "        # print(targets, \"targets\")\n",
    "        for i in targets[k[0]]:\n",
    "            total_1 += calculate_distance(directed_graphs[k[0]], k[1], i)\n",
    "            total_2 += calculate_distance(undirected_graphs[k[0]], k[1], i)\n",
    "        # except:\n",
    "        #     continue\n",
    "        dis_directed[k] = total_1\n",
    "        dis_undirected[k] = total_2\n",
    "    \n",
    "    # print(dis_directed)\n",
    "    if not len(dis_directed):\n",
    "        return \"\"\n",
    "\n",
    "    lowest_key = get_lowest_key(dis_directed)\n",
    "\n",
    "    if len(lowest_key) == 1:\n",
    "        return row[key_candidate][lowest_key[0]]\n",
    "\n",
    "    lowest_key = get_lowest_key(dis_undirected)\n",
    "    if len(lowest_key) == 1:\n",
    "        return row[key_candidate][lowest_key[0]]\n",
    "    else:\n",
    "        # print(lowest_key)\n",
    "        return \"not implemented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "9e493e86-bd07-4dc0-97af-d9fd338cc2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_answer_dep(row, key_candidate, key_entities, key_np):\n",
    "    idx_target_entities = get_target_idx(row, key_candidate, key_entities)\n",
    "    idx_target_np = get_target_idx(row, key_candidate, key_np)\n",
    "    target_entities = row[key_entities]\n",
    "    target_np = row[key_np]\n",
    "    target_all = target_entities + target_np\n",
    "    idx_root = {}\n",
    "    q_root = \"\"\n",
    "    \n",
    "    directed_graphs = {}\n",
    "    undirected_graphs = {}\n",
    "    filter_by_label = row[\"filter_by_label\"]\n",
    "    # filter_by_label = filter_by_label.pop((-1, -1, -1))\n",
    "    \n",
    "    if len(row[key_candidate]) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    for k, v in row[key_candidate].items():\n",
    "        if k[0] < 0:\n",
    "            continue\n",
    "        for i, j in enumerate(row[\"dependencies\"][k[0]]):\n",
    "            if j[2] == \"root\":\n",
    "                idx_root[k[0]] = i\n",
    "        directed_graphs[k[0]] = create_directed_graph(row[\"dependencies\"][k[0]])\n",
    "        undirected_graphs[k[0]] = create_undirected_graph(row[\"dependencies\"][k[0]])\n",
    "    \n",
    "    for kata in row[\"q_dependencies\"]:\n",
    "        if kata[2] == \"root\":\n",
    "            q_root = kata[1]\n",
    "            break\n",
    "    \n",
    "            \n",
    "    idx_target_from_q = get_target_idx_2(row, key_candidate, key_entities, q_root)\n",
    "    \n",
    "    dis_directed = {}\n",
    "    dis_undirected = {}\n",
    "    \n",
    "    #11111111\n",
    "    for k, v in filter_by_label.items():\n",
    "        jaraks = []\n",
    "        found_overlap = False\n",
    "        for en in target_entities:\n",
    "           \n",
    "            if remove_punctuation(en).lower().strip() in v.lower():\n",
    "                found_overlap = True\n",
    "                break\n",
    "                \n",
    "        if found_overlap:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for i in idx_target_entities[k[0]]:\n",
    "                for j in range(k[1], k[2]):\n",
    "                    jaraks.append(calculate_distance(directed_graphs[k[0]], i, j))\n",
    "        except KeyError:\n",
    "            continue\n",
    "           \n",
    "        dis_directed[k] = min(jaraks)\n",
    "        \n",
    "    # print(dis_directed)\n",
    "    \n",
    "    if dis_directed:\n",
    "        lowest_key = get_lowest_key(dis_directed)\n",
    "        if len(lowest_key) == 1 and dis_directed[lowest_key[0]] != 9999:\n",
    "            return row[key_candidate][lowest_key[0]]\n",
    "    \n",
    "    #22222222\n",
    "    for k, v in filter_by_label.items():\n",
    "        jaraks = []\n",
    "        found_overlap = False\n",
    "        for en in target_np:\n",
    "            if remove_punctuation(en).lower().strip() in v.lower():\n",
    "                # print(remove_punctuation(en).lower().strip(), v.lower())\n",
    "                found_overlap = True\n",
    "                break\n",
    "                \n",
    "        if found_overlap:\n",
    "            continue\n",
    "        try:\n",
    "            for i in idx_target_np[k[0]]:\n",
    "                for j in range(k[1], k[2]):\n",
    "                    jaraks.append(calculate_distance(directed_graphs[k[0]], i, j))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        dis_directed[k] = min(jaraks)\n",
    "    # print(dis_directed)\n",
    "    \n",
    "    \n",
    "    if dis_directed:\n",
    "        lowest_key = get_lowest_key(dis_directed)\n",
    "        if len(lowest_key) == 1 and dis_directed[lowest_key[0]] != 9999:\n",
    "            return row[key_candidate][lowest_key[0]]\n",
    "    \n",
    "    # 222.111\n",
    "    for k, v in filter_by_label.items():\n",
    "        jaraks = []\n",
    "        # found_overlap = False\n",
    "#         for en in target_from_q:\n",
    "#             if remove_punctuation(en).lower().strip() in v.lower():\n",
    "#                 # print(remove_punctuation(en).lower().strip(), v.lower())\n",
    "#                 found_overlap = True\n",
    "#                 break\n",
    "                \n",
    "#         if found_overlap:\n",
    "#             continue\n",
    "        try:\n",
    "            for i in idx_target_from_q[k[0]]:\n",
    "                for j in range(k[1], k[2]):\n",
    "                    jaraks.append(calculate_distance(directed_graphs[k[0]], i, j))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        dis_directed[k] = min(jaraks)\n",
    "    # print(dis_directed)\n",
    "    \n",
    "    \n",
    "    if dis_directed:\n",
    "        lowest_key = get_lowest_key(dis_directed)\n",
    "        if len(lowest_key) == 1 and dis_directed[lowest_key[0]] != 9999:\n",
    "            return row[key_candidate][lowest_key[0]]\n",
    "    \n",
    "    # 333333333\n",
    "    for k, v in filter_by_label.items():\n",
    "        if k[0] < 0:\n",
    "            continue\n",
    "        jaraks = []\n",
    "        found_overlap = True\n",
    "        for en in target_all:\n",
    "            # Kalau semua kata pada entitas ada di answer\n",
    "            if remove_punctuation(en).lower().strip() not in v.lower():\n",
    "                found_overlap = False\n",
    "                break\n",
    "            # if remove_punctuation(en).lower().strip() in v.lower():\n",
    "            #     # print(remove_punctuation(en).lower().strip(), v.lower())\n",
    "            #     found_overlap = True\n",
    "            #     break\n",
    "                \n",
    "        if found_overlap:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for j in range(k[1], k[2]):\n",
    "                jaraks.append(calculate_distance(directed_graphs[k[0]], idx_root[k[0]], j))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        dis_directed[k] = min(jaraks)\n",
    "        \n",
    "    # print(dis_directed)\n",
    "    \n",
    "    if dis_directed:\n",
    "        lowest_key = get_lowest_key(dis_directed)\n",
    "        if len(lowest_key) == 1 and dis_directed[lowest_key[0]] != 9999:\n",
    "            return row[key_candidate][lowest_key[0]]\n",
    "        \n",
    "    #444444    \n",
    "    for k, v in filter_by_label.items():\n",
    "        jaraks = []\n",
    "        found_overlap = False\n",
    "        for en in target_entities:\n",
    "            if remove_punctuation(en).lower().strip() in v.lower():\n",
    "                found_overlap = True\n",
    "                break\n",
    "                \n",
    "        if found_overlap:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for i in idx_target_entities[k[0]]:\n",
    "                for j in range(k[1], k[2]):\n",
    "                    jaraks.append(calculate_distance(directed_graphs[k[0]], j, i))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        dis_directed[k] = min(jaraks)\n",
    "    # print(dis_directed)\n",
    "    \n",
    "    ### 55555q\n",
    "    if dis_directed:\n",
    "        lowest_key = get_lowest_key(dis_directed)\n",
    "        if len(lowest_key) == 1 and dis_directed[lowest_key[0] ]!= 9999:\n",
    "            return row[key_candidate][lowest_key[0]]\n",
    "    \n",
    "    for k, v in filter_by_label.items():\n",
    "        jaraks = []\n",
    "        found_overlap = False\n",
    "        for en in target_np:\n",
    "           \n",
    "            if remove_punctuation(en).lower().strip() in v.lower():\n",
    "                # print(remove_punctuation(en).lower().strip(), v.lower())\n",
    "                found_overlap = True\n",
    "                break\n",
    "                \n",
    "        if found_overlap:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            for i in idx_target_np[k[0]]:\n",
    "                for j in range(k[1], k[2]):\n",
    "                    jaraks.append(calculate_distance(directed_graphs[k[0]], j, i))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        dis_directed[k] = min(jaraks)\n",
    "        \n",
    "    # print(dis_directed)\n",
    "    \n",
    "    if dis_directed:\n",
    "        lowest_key = get_lowest_key(dis_directed)\n",
    "        if len(lowest_key) == 1 and dis_directed[lowest_key[0]] != 9999:\n",
    "            return row[key_candidate][lowest_key[0]]\n",
    "    \n",
    "    return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194b648-4375-4699-9c6d-0f85ed546b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squad_test[\"ans_dep\"] = squad_test.progress_apply(lambda row: get_answer_dep(row, \"filter_by_label\", \"content_entities\", \"content_np\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b29ff-8885-4472-9b59-52fc003673cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tydiqa_test[\"ans_dep\"] = tydiqa_test.progress_apply(lambda row: get_answer_dep(row, \"filter_by_label\", \"content_entities\", \"content_np\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59660691-a943-4a87-bf6e-eed860bac901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idkmrc_test[\"ans_dep\"] = idkmrc_test.progress_apply(lambda row: get_answer_dep(row, \"filter_by_label\", \"content_entities\", \"content_np\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a9e5d-5106-4118-98d7-6c6ac1b9dcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idkmrc_test[\"filter_by_label\"] = idkmrc_test.progress_apply(lambda row: get_answer(row, \"filtered_candidate_answer_entities_np_or\", \"content_np\"), axis=1)\n",
    "tydiqa_test[\"filter_by_label\"] = tydiqa_test.progress_apply(lambda row: get_answer(row, \"filtered_candidate_answer_entities_np_or\", \"content_np\"), axis=1)\n",
    "squad_test[\"filter_by_label\"] = squad_test.progress_apply(lambda row: get_answer(row, \"filtered_candidate_answer_entities_np_or\", \"content_np\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "bccd06e5-fbd2-40d0-8672-ccfdcc2ac45d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filtered_not_null(df, key):\n",
    "    temp_df = df[df[key].apply(lambda x: len(x) != 0)]\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6f16a-fd13-49e3-a65d-2488b1a4511e",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "c3add1eb-5203-422c-be6d-e8fc8931e4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_answer_ts(row, debug = False):\n",
    "    try:\n",
    "        passage = row[\"passage\"]\n",
    "    except KeyError:\n",
    "        passage = row[\"context\"]\n",
    "    \n",
    "    passage_sentences = split_sentences(passage)\n",
    "    \n",
    "    candidate_sentences = passage_sentences\n",
    "\n",
    "    question_pos_wo_wh = filter_wh([[w[\"kata\"], w[\"pos\"]] for w in row[\"question_features\"][0]])\n",
    "    candidate_answer = row[\"filter_by_label\"]\n",
    "    \n",
    "    # Get eat\n",
    "    kata_tanya = row[\"all_question\"]\n",
    "    eat = row[\"eat_4\"]\n",
    "    \n",
    "    question_key = get_question_key(question_pos_wo_wh)\n",
    "    \n",
    "    final_answer = get_final_answer(passage_sentences, kata_tanya, question_key, candidate_answer, debug)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c217bb0-7001-482f-9a1f-623f287befcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squad_test[\"ans_ts\"] = squad_test.progress_apply(lambda row: get_answer_ts(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc91c52-a23e-466d-9442-670f7ed204db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tydiqa_test[\"ans_ts\"] = tydiqa_test.progress_apply(lambda row: get_answer_ts(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba29e6b-ebbd-4b79-ac96-f6ef4a5777e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idkmrc_test[\"ans_ts\"] = idkmrc_test.progress_apply(lambda row: get_answer_ts(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ce2a8-f5de-489c-9372-272e183a3b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_all(squad_test, \"ans_ts\")\n",
    "eval_all(tydiqa_test, \"ans_ts\")\n",
    "eval_all(idkmrc_test, \"ans_ts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
